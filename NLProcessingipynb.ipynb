{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJZeEf8UWFE8",
        "outputId": "e22871b5-be50-4ae9-fadc-32b20b2a748d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = \"\"\"\n",
        "Stella, a software engineer with a fire in her eyes, slammed her laptop shut. Her latest code, a groundbreaking algorithm, refused to compile. Hours of meticulous work seemed on the verge of collapse. Was she just tilting at windmills, her human effort insignificant against the unforgiving logic of the machine?\n",
        "\n",
        "\"Even the most elegant code can't predict every twist,\" said a gentle voice. David, a senior developer with a calming presence, stood beside her.\n",
        "\n",
        "Stella scoffed. \"So, are we all just at the mercy of random bugs and glitches?\"\n",
        "\n",
        "David chuckled. \"Think of it like this. You write the code (agency), you craft the logic (action), but sometimes unforeseen complexities (external forces) arise. A skilled programmer trusts their abilities but also acknowledges the inherent mysteries of the digital world.\"\n",
        "\n",
        "Stella felt a knot loosen in her chest. Maybe her frustration stemmed from the illusion of total control. Perhaps her role was to leverage her skills and knowledge while embracing the unpredictable nature of the digital realm. It was about finding the bridge between determined effort and accepting the larger design of the system's intricate workings.  She took a deep breath, a renewed sense of purpose sparking in her eyes. There was still a solution waiting to be found, a harmonious dance between her will and the hidden logic of the code.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "W77nED14WWCf"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization\n",
        "\n",
        "## 1. Sentence Tokenization\n",
        "\n",
        "## 2. Word Tokenization"
      ],
      "metadata": {
        "id": "gEoqFG8qZLaX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 1. Senetence Tokenization"
      ],
      "metadata": {
        "id": "NU1yshu_ZQuQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "sentences = sent_tokenize(corpus)\n",
        "for x in sentences:\n",
        "  print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cjuNYerZU58",
        "outputId": "69895906-b89c-452c-e24e-06390e7cf74b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Stella, a software engineer with a fire in her eyes, slammed her laptop shut.\n",
            "Her latest code, a groundbreaking algorithm, refused to compile.\n",
            "Hours of meticulous work seemed on the verge of collapse.\n",
            "Was she just tilting at windmills, her human effort insignificant against the unforgiving logic of the machine?\n",
            "\"Even the most elegant code can't predict every twist,\" said a gentle voice.\n",
            "David, a senior developer with a calming presence, stood beside her.\n",
            "Stella scoffed.\n",
            "\"So, are we all just at the mercy of random bugs and glitches?\"\n",
            "David chuckled.\n",
            "\"Think of it like this.\n",
            "You write the code (agency), you craft the logic (action), but sometimes unforeseen complexities (external forces) arise.\n",
            "A skilled programmer trusts their abilities but also acknowledges the inherent mysteries of the digital world.\"\n",
            "Stella felt a knot loosen in her chest.\n",
            "Maybe her frustration stemmed from the illusion of total control.\n",
            "Perhaps her role was to leverage her skills and knowledge while embracing the unpredictable nature of the digital realm.\n",
            "It was about finding the bridge between determined effort and accepting the larger design of the system's intricate workings.\n",
            "She took a deep breath, a renewed sense of purpose sparking in her eyes.\n",
            "There was still a solution waiting to be found, a harmonious dance between her will and the hidden logic of the code.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 2. Word Tokenization"
      ],
      "metadata": {
        "id": "RkBPCQyHe6kd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "words = word_tokenize(\"Hello, friend. Hello, friend? That's lame. Maybe I should give you a name, but that's a slippery slope. You're only in my head. We have to remember that.\")\n",
        "for x in words:\n",
        "  print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_lQF9a4Zpip",
        "outputId": "3b409a4e-a372-47fe-b8da-720871f92a21"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello\n",
            ",\n",
            "friend\n",
            ".\n",
            "Hello\n",
            ",\n",
            "friend\n",
            "?\n",
            "That\n",
            "'s\n",
            "lame\n",
            ".\n",
            "Maybe\n",
            "I\n",
            "should\n",
            "give\n",
            "you\n",
            "a\n",
            "name\n",
            ",\n",
            "but\n",
            "that\n",
            "'s\n",
            "a\n",
            "slippery\n",
            "slope\n",
            ".\n",
            "You\n",
            "'re\n",
            "only\n",
            "in\n",
            "my\n",
            "head\n",
            ".\n",
            "We\n",
            "have\n",
            "to\n",
            "remember\n",
            "that\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stemming\n",
        "\n",
        "## 1. Porter Stemming\n",
        "\n",
        "## 2. Regexp Stemming\n",
        "\n",
        "## 3. Snowball Stemming"
      ],
      "metadata": {
        "id": "xDoFFp4cRgpl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 1. Porter Stemmer"
      ],
      "metadata": {
        "id": "uYu25huiTNwb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "words = word_tokenize(\"Hello, friend. Hello, friend? That's lame. Maybe I should give you a name, but that's a slippery slope. You're only in my head. We have to remember that.\")\n",
        "\n",
        "ps = PorterStemmer()\n",
        "for x in words:\n",
        "  print(ps.stem(x), end=\" \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULohZXgniFWk",
        "outputId": "e04e6656-c0b9-4977-c0aa-908ceedafc0a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello , friend . hello , friend ? that 's lame . mayb i should give you a name , but that 's a slipperi slope . you 're onli in my head . we have to rememb that . "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 2. Regexp Stemming"
      ],
      "metadata": {
        "id": "S8r-yyyGUIe9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import RegexpStemmer\n",
        "\n",
        "words = word_tokenize(\"Hello, friend. Hello, friend? That's lame. Maybe I should give you a name, but that's a slippery slope. You're only in my head. We have to remember that.\")\n",
        "\n",
        "rs = RegexpStemmer('ing$|able$|s$')\n",
        "for x in words:\n",
        "  print(rs.stem(x), end=\" \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tothpl7YTfrl",
        "outputId": "de80e25c-6c09-4d57-da53-8d8993913cc4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello , friend . Hello , friend ? That ' lame . Maybe I should give you a name , but that ' a slippery slope . You 're only in my head . We have to remember that . "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 3. Snowball Stemming"
      ],
      "metadata": {
        "id": "E66JYudRU8My"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "words = word_tokenize(\"Hello, friend. Hello, friend? That's lame. Maybe I should give you a name, but that's a slippery slope. You're only in my head. We have to remember that.\")\n",
        "\n",
        "ss = SnowballStemmer(\"english\")\n",
        "for x in words:\n",
        "  print(ss.stem(x), end=\" \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UL_lDwl3UiWo",
        "outputId": "4e2fac4b-9fdd-4d93-9a85-f8d6566dd07a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello , friend . hello , friend ? that 's lame . mayb i should give you a name , but that 's a slipperi slope . you re onli in my head . we have to rememb that . "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lemmatization\n",
        "\n",
        "## 1. Wordnet Lemmatization"
      ],
      "metadata": {
        "id": "_yd9Y2gBMLV9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "words = word_tokenize(\"Hello, friend. Hello, friend? That's lame. Maybe I should give you a name, but that's a slippery slope. You're only in my head. We have to remember that.\")\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "for x in words:\n",
        "  print(lemmatizer.lemmatize(x, pos=\"v\"), end=\" \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "caj-sSygU3yW",
        "outputId": "4a685883-3f75-434e-9e3b-53c6f0cd90cd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello , friend . Hello , friend ? That 's lame . Maybe I should give you a name , but that 's a slippery slope . You 're only in my head . We have to remember that . "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stop words removal"
      ],
      "metadata": {
        "id": "XxwgG1oCOKtv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "words = word_tokenize(\"Hello, friend. Hello, friend? That's lame. Maybe I should give you a name, but that's a slippery slope. You're only in my head. We have to remember that.\")\n",
        "\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "for x in words:\n",
        "  if x not in stop_words:\n",
        "    print(x, end=\" \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBpQMVk5MrrK",
        "outputId": "0822b8cf-fb78-4d2d-89c6-efc2681c85a0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello , friend . Hello , friend ? That 's lame . Maybe I give name , 's slippery slope . You 're head . We remember . "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parts of Speech Tagging (POS)"
      ],
      "metadata": {
        "id": "J3iMy3TRQjuq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "words = word_tokenize(\"Hello, friend. Hello, friend? That's lame. Maybe I should give you a name, but that's a slippery slope. You're only in my head. We have to remember that.\")\n",
        "\n",
        "for x in words:\n",
        "  print(x, nltk.pos_tag([x]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bi-fSVSJOilG",
        "outputId": "2b91f5a6-dd11-4d1d-d485-6fc7d7ecf2ce"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello [('Hello', 'NN')]\n",
            ", [(',', ',')]\n",
            "friend [('friend', 'NN')]\n",
            ". [('.', '.')]\n",
            "Hello [('Hello', 'NN')]\n",
            ", [(',', ',')]\n",
            "friend [('friend', 'NN')]\n",
            "? [('?', '.')]\n",
            "That [('That', 'DT')]\n",
            "'s [(\"'s\", 'POS')]\n",
            "lame [('lame', 'NN')]\n",
            ". [('.', '.')]\n",
            "Maybe [('Maybe', 'RB')]\n",
            "I [('I', 'PRP')]\n",
            "should [('should', 'MD')]\n",
            "give [('give', 'VB')]\n",
            "you [('you', 'PRP')]\n",
            "a [('a', 'DT')]\n",
            "name [('name', 'NN')]\n",
            ", [(',', ',')]\n",
            "but [('but', 'CC')]\n",
            "that [('that', 'IN')]\n",
            "'s [(\"'s\", 'POS')]\n",
            "a [('a', 'DT')]\n",
            "slippery [('slippery', 'NN')]\n",
            "slope [('slope', 'NN')]\n",
            ". [('.', '.')]\n",
            "You [('You', 'PRP')]\n",
            "'re [(\"'re\", 'VBP')]\n",
            "only [('only', 'RB')]\n",
            "in [('in', 'IN')]\n",
            "my [('my', 'PRP$')]\n",
            "head [('head', 'NN')]\n",
            ". [('.', '.')]\n",
            "We [('We', 'PRP')]\n",
            "have [('have', 'VB')]\n",
            "to [('to', 'TO')]\n",
            "remember [('remember', 'VB')]\n",
            "that [('that', 'IN')]\n",
            ". [('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hBpDW1ZrRedz"
      },
      "execution_count": 10,
      "outputs": []
    }
  ]
}